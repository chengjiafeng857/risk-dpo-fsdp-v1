policy_model: meta-llama/Llama-3.1-8B-Instruct
ref_model: meta-llama/Llama-3.1-8B-Instruct
precision: bf16

dataset:
  dataset_name: Anthropic/hh-rlhf
  subset: train[:100%]
  val_ratio: 0.1
  seed: 42
  max_len: 512
  
dpo_training:
  epochs: 1
  batch_size: 16
  learning_rate: 5e-7
  log_steps: 10
  warmup_steps: 60
  max_grad_norm: 10
  dpo_beta: 0.5
  save_dir: dpo_model

tail_test:
  delta: 0.1
  lambda: 0.05
  log_dir: logs/margins

HH_test:
  hh_test: Anthropic/hh-rlhf
  seed: 42
  num_sample: 500
  max_prompt_length: 512
  max_new_tokens: 256
  temperature: 1.0
  top_p: 0.9
  do_sample: true
  HH_test_policy_out: eval_outputs/policy_out.jsonl
  HH_test_base_out: eval_outputs/base_out.jsonl

vllm:
  tensor_parallel_size: 1
  chunk_size: 32